# 은닉층 개수와 뉴런 개수에 따른 정확성 연관 관계

### 1. 서론

#### 
딥러닝은 머신 러닝의 한 분야로서 데이터로부터 표현을 학습하는 새로운 방식이다. 딥러닝의 딥이란 연속된 층으로 표현을 학습한다는 개념을 나타낸다. 데이터로부터 모델을 만드는 데 얼마나 많은 층을 사용했는지가 그 모델의 깊이가 된다. 최근의 딥러닝 모델은 표현 학습을 위해 수십 개, 수백 개의 연속된 층을 가지고 있다. 

#### 
이러한 딥러닝의 층은 여러 개의 인공뉴런으로 구성되고 있으며 인공 뉴런을 노드라고 부른다. 노드의 기능은 단순히 입력된 신호 x=[x1, x2, … , xn]을 연결가중치 w=[w1, w2, … , wn]과 곱한 값을 모두 더한 다음, 그 결과에 비선형 함수 f를 취하는 것이다. 
#### 
본 보고서에서는 딥러닝의 은닉층의 개수에 따라 정확도가 어떻게 변화는지 분석하였다.

### 2. 은닉층에 따른 정확도, 오류의 영향
#### 
본 실험은 은닉층 개수 증가에 따라 정확도의 변화를 분석하였다. TensorFlow의 Mnist 데이터 셋을 이용하여 모델이 이미지를 보고 어떤 숫자인지 예측하는 모델을 사용한다. 훈련 데이터 60,000개를 통해 모델을 훈련 시키고 10,000개의 테스트 데이터를 통해 결과를 도출했다. 모든 은닉층은 256개의 노드를 가지고 있다.

![image](https://user-images.githubusercontent.com/79610047/150049776-2924f60b-de0d-49d4-b8fe-ff25826517f4.png)

#### 
은닉층의 개수가 25개까지는 정확도의 변화가 거의 없다. 하지만, 26개에서 27개로 가는 구간에서 정확도가 81%에서 11%로 급격히 감소한다. 

### 3. 실험 결과 
####
실험 결과, 은닉층 개수가 증가할수록 정확도가 증가하다가 27개부터 급격히 감소했다. 이는 출력층에서 멀어지면 멀어질수록 신경망의 오차가 반영되지 않아 발생하는 현상이다. 앞쪽의 은닉층까지는 오차가 거의 전달되지 않으니 가중치도 변하지 않게 되어 입력층에 가까운 은닉층들은 제대로 학습되지 않는다. 학습이 제대로 되지 않은 은닉층을 추가하면 아무런 의미가 없다. 그리고 은닉층이 많아지면 복잡성이 증가하고 훈련데이터를 반영하는 모델을 만들 수 있겠지만, 과대적합의 가능성 역시 높아지며, 훈련 속도도 느려진다. 
####
본 실험을 통해 필요 이상으로 은닉층이 증가할수록 과대적합, 그래디언트 소멸 현상이 발생된다는 것을 알 수 있다. 
### 4. 결론
#### 
본 보고서를 통해 특정 개수 이상의 은닉층은 신경망 성능 저하의 원인이 된다. 과대적합, 그래디언 소멸 현상을 해결하기 위해서 성능 좋은 신경망을 만들기 위해서는 적합한 은닉층 개수를 찾는 것이 중요하다. 또한, 은닉층의 뉴런 수는 각 층의 뉴런을 점점 줄여가는 방법으로 모델링한다. 그 이유는 저 수준의 많은 특성이 고수준의 적은 특성으로 합쳐질 수 있기 때문이다. 
####
결론적으로, 성능 좋은 신경망을 만들기 위해서는 적합한 은닉층 개수를 찾는 것이 중요하다.  
